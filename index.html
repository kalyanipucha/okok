
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Inclusive AI App - Updated (Back Camera)</title>
    <style>
        /* Add styles here (same as your existing styles) */
        body {
            font-family: 'Roboto', sans-serif;
            margin: 0;
            padding: 0;
            background: #1a1a2e;
            color: #e6e6e6;
            height: 100%;
            display: flex;
            flex-direction: column;
            justify-content: space-between;
        }

        header {
            background: linear-gradient(to right, #0f2027, #203a43, #2c5364);
            color: #00d4ff;
            padding: 40px 20px;
            text-align: center;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
            text-transform: uppercase;
            letter-spacing: 2px;
        }

        header h1 {
            font-size: 2.5rem;
        }

        .container {
            max-width: 1200px;
            margin: auto;
            padding: 20px;
            flex-grow: 1;
        }

        section {
            margin: 30px 0;
            padding: 20px;
            background: #21253a;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);
        }

        section h2 {
            color: #00ffab;
            font-size: 2rem;
            text-align: center;
        }

        .feature {
            margin: 15px 0;
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .feature input,
        .feature button,
        .feature textarea {
            margin: 10px 0;
            padding: 15px;
            border-radius: 5px;
            border: 1px solid #00d4ff;
            background: #2d3448;
            color: #e6e6e6;
            width: 100%;
            max-width: 500px;
        }

        button {
            background-color: #00d4ff;
            color: #1a1a2e;
            cursor: pointer;
            transition: all 0.3s ease;
            font-weight: bold;
            font-size: 1.2rem;
            padding: 15px;
        }

        button:hover {
            background-color: #00ffab;
            transform: scale(1.05);
        }

        .main-buttons {
            display: flex;
            justify-content: space-around;
            margin: 50px 0;
        }

        .main-buttons button {
            font-size: 1.5rem;
            padding: 20px;
            background-color: #00d4ff;
            border: none;
            width: 30%;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
            transition: all 0.3s ease;
        }

        .main-buttons button:hover {
            background-color: #00ffab;
            transform: scale(1.1);
        }

        footer {
            text-align: center;
            padding: 15px;
            background: #0f2027;
            color: #00d4ff;
            font-size: 0.9rem;
        }

        .instructions {
            font-style: italic;
            color: #888;
            margin-top: 5px;
        }

        video {
            width: 70%;
            margin-top: 20px;
            border-radius: 12px;
        }

        .btn {
            margin: 10px;
            padding: 12px 22px;
            font-size: 18px;
            border: none;
            border-radius: 8px;
            background: #00e5ff;
            cursor: pointer;
        }

        nav a {
            color: rgba(255, 255, 255, 0.806);
            text-decoration: underline;
            margin-left: 1.5rem;
            font-weight: 500;
            transition: color 0.3s;
        }

        nav a:hover {
            color: var(--lime);
        }
    </style>
</head>

<body>
    <header>
        <h1>Vita AI</h1>
        <p>Empowering Accessibility Through Technology</p>
    </header>

    <div class="container">
        <!-- Main Buttons for Navigation -->
        <div class="main-buttons">
            <button onclick="navigateTo('blind'), stopAudio()">For Blind Users</button>
            <button onclick="navigateTo('mute'), stopAudio()">For Mute Users</button>
            <button onclick="navigateTo('deaf'), stopAudio()">For Deaf Users</button>
        </div>

        <!-- Blind Users Section -->
        <section id="blind" style="display:none;">
            <h2>For Blind Users</h2>
            <p>Select a function to assist you.</p>
            <div class="feature">
                <button onclick="showImageRecognition()" aria-label="Image Recognition">Image Recognition</button>
                <button onclick="showTextRecognition()" aria-label="Text Recognition">Text Recognition</button>
                <button onclick="getRealTimeLocation();hideOther()" aria-label="Get Real-Time Location">Get Real-Time Location</button>
                <p id="locationResponse"></p>
                <button onclick="showRealTimeDetection();hideOther()" aria-label="Get Real-Time Detection">Live vision detection</button>
            </div>

            <!-- Image Recognition Section -->
            <div id="imageRecognitionSection" style="display:none;">
                <h3>Image Recognition</h3>
                <p>Upload an image to hear its description.</p>
                <div class="feature">
                    <input type="file" id="imageUpload" accept="image/*" aria-label="Upload an image">
                    <button onclick="analyzeImage()" aria-label="Analyze image">Analyze Image</button>
                    <button onclick="stopAudio()" aria-label="Stop Audio">Stop Audio</button>
                    <p class="instructions">Upload a clear image for the best results.</p>
                </div>
                <p id="imageResponse"></p>
            </div>

            <!-- Text Recognition Section -->
            <div id="textRecognitionSection" style="display:none;">
                <h3>Text Recognition</h3>
                <p>Extract text from an image.</p>
                <div class="feature">
                    <input type="file" id="textImageUpload" accept="image/*" aria-label="Upload an image with text">
                    <button onclick="extractTextFromImage()" aria-label="Extract Text">Extract Text</button>
                    <button onclick="stopAudio()" aria-label="Stop Audio">Stop Audio</button>
                    <p class="instructions">Upload an image with visible text for extraction.</p>
                </div>
                <p id="textResponse"></p>
            </div>

            <!-- Video Recognition Section -->
            <div id="videoRecognitionSection" style="display:none;">
                <h3>Advanced Live Google Vision Describer</h3>
                <div class="feature">
                    <video id="video" autoplay playsinline></video><br>
                    <div style="display:flex; flex-wrap:wrap; justify-content:center;">
                        <button class="btn" id="startBtn">Start Describing</button>
                        <button class="btn" id="stopBtn">Stop Describing</button>
                        <!-- NEW: Back camera switch button (user requested) -->
                        <button class="btn" id="backCamBtn" title="Switch to back camera">Use Back Camera</button>
                        <button class="btn" id="frontCamBtn" title="Switch to front camera">Use Front Camera</button>
                    </div>
                </div>
            </div>
        </section>

        <!-- Mute Users Section -->
        <section id="mute" style="display:none;">
            <h2>For Mute Users</h2>
            <p>Type your message, and let the app speak it for you.</p>
            <div class="feature">
                <label for="languageSelectMute">Choose Language:</label>
                <select id="languageSelectMute">
                    <option value="en-US">English (US)</option>
                    <option value="es-ES">Spanish</option>
                    <option value="fr-FR">French</option>
                    <option value="de-DE">German</option>
                    <option value="hi-IN">Hindi</option>
                </select>
                <textarea id="textInput" placeholder="Type your message here..." aria-label="Message input"></textarea>
                <button onclick="convertTextToSpeech()" aria-label="Speak message">Speak Message</button>
                <button onclick="showMessages(), stopAudio()" aria-label="Quick Messages">Quick Messages</button>
            </div>
            <section id="preset-messages" style="display:none;">
                <h2>Quick Messages</h2>
                <button onclick="presetSpeak('Hello, how can I help you?')">Hello, how can I help you?</button>
                <button onclick="presetSpeak('Good Morning!')">Good Morning!</button><br><br>
                <button onclick="presetSpeak('I need assistance.')">I need assistance.</button>
                <button onclick="presetSpeak('Thank you!')">Thank you!</button>
                <button onclick="presetSpeak('Excuse Me please')">Excuse Me please</button><br><br><br>
                <div>
                    <button style="align-self: center;background-color: #8fc5ce; " onclick="backMessages()" aria-label="Back">Back</button>
                </div>
            </section>
        </section>

        <!-- Deaf Users Section -->
        <section id="deaf" style="display:none;">
            <h2>For Deaf Users</h2>
            <p>Convert spoken language into text in real-time.</p>
            <div class="feature">
                <label for="languageSelectDeaf">Choose Language:</label>
                <select id="languageSelectDeaf">
                    <option value="en-US">English (US)</option>
                    <option value="es-ES">Spanish</option>
                    <option value="fr-FR">French</option>
                    <option value="de-DE">German</option>
                    <option value="hi-IN">Hindi</option>
                </select>
                <button onclick="startSpeechToText()" aria-label="Start speech-to-text">Start Speech-to-Text</button>
                <button onclick="stopSpeechToText()" aria-label="Stop speech-to-text">Stop Speech-to-Text</button>
                <p id="speechOutput"></p>
            </div>
        </section>
    </div>

    <footer>
        <nav>
            <a href="contact support.html">Contact Support</a>
            <a href="how to use.html">How to Use</a>
        </nav>
        <p>&copy; 2025 Vita AI. Designed for accessibility and innovation.</p>
        <p>Created with dedication by <strong>Milind Vijay Phadatare</strong>, <strong>Pratik Rajendra Nagargoje</strong>, and <strong>Kalyani Balaraju Pucha</strong>.</p>
    </footer>

    <script>
        // Function to navigate to different sections
        function navigateTo(section) {
            document.getElementById('blind').style.display = 'none';
            document.getElementById('mute').style.display = 'none';
            document.getElementById('deaf').style.display = 'none';
            document.getElementById(section).style.display = 'block';
        }

        // Show Image Recognition section
        function showImageRecognition() {
            document.getElementById('imageRecognitionSection').style.display = 'block';
            document.getElementById('textRecognitionSection').style.display = 'none';
            document.getElementById('videoRecognitionSection').style.display = 'none';
        }

        // Show Text Recognition section
        function showTextRecognition() {
            document.getElementById('textRecognitionSection').style.display = 'block';
            document.getElementById('imageRecognitionSection').style.display = 'none';
            document.getElementById('videoRecognitionSection').style.display = 'none';
        }

        function showRealTimeDetection() {
            document.getElementById('imageRecognitionSection').style.display = 'none';
            document.getElementById('textRecognitionSection').style.display = 'none';
            document.getElementById('videoRecognitionSection').style.display = 'block';

            startCamera(); // ✅ camera ONLY here
        }

        // Show Quick Messages section
        function showMessages() {
            document.getElementById('preset-messages').style.display = 'block';
            document.getElementById('textRecognitionSection').style.display = 'none';
        }

        function backMessages() {
            document.getElementById('mute').style.display = 'block';
            document.getElementById('preset-messages').style.display = 'none';
        }

        function hideOther() {
            document.getElementById('imageRecognitionSection').style.display = 'none';
            document.getElementById('textRecognitionSection').style.display = 'none';
        }

        // Analyze Image for Blind Users
        async function analyzeImage() {
            const fileInput = document.getElementById("imageUpload");
            const responseElement = document.getElementById("imageResponse");

            if (fileInput.files.length === 0) {
                responseElement.textContent = "Please upload an image.";
                speakText("Please upload an image.");
                return;
            }

            // Implement API calls for multilingual support (same as original with language parameter)
        }

        // Analyze Image for Blind Users using Google Cloud Vision API
        async function analyzeImage() {
            const fileInput = document.getElementById("imageUpload");
            const responseElement = document.getElementById("imageResponse");

            if (fileInput.files.length === 0) {
                responseElement.textContent = "Please upload an image.";
                speakText("Please upload an image.");
                return;
            }

            const file = fileInput.files[0];
            const reader = new FileReader();

            reader.onload = async function () {
                const base64Image = reader.result.split(",")[1];

                const requestBody = {
                    requests: [
                        {
                            image: { content: base64Image },
                            features: [{ type: "LABEL_DETECTION", maxResults: 10 }]
                        }
                    ]
                };

                const apiKey = "AIzaSyA0US6rM_BBm-OypJkKfOyn7uTfatnbw"; // Your API Key here
                const apiUrl = `https://vision.googleapis.com/v1/images:annotate?key=${apiKey}`;

                responseElement.textContent = "Processing image...";
                speakText("Processing image...");

                try {
                    const response = await fetch(apiUrl, {
                        method: "POST",
                        headers: {
                            "Content-Type": "application/json"
                        },
                        body: JSON.stringify(requestBody)
                    });

                    if (!response.ok) {
                        throw new Error("Failed to analyze image.");
                    }

                    const data = await response.json();
                    if (data.responses[0].labelAnnotations && data.responses[0].labelAnnotations.length > 0) {
                        const labels = data.responses[0].labelAnnotations.map(label => label.description).join(', ');
                        responseElement.textContent = `Image contains: ${labels}`;
                        speakText(`The image contains: ${labels}`);
                    } else {
                        throw new Error("No recognizable objects found.");
                    }
                } catch (error) {
                    responseElement.textContent = error.message;
                    speakText(error.message);
                }
            };

            reader.readAsDataURL(file);
        }

        // Extract Text from Image for Blind Users
        async function extractTextFromImage() {
            const fileInput = document.getElementById("textImageUpload");
            const responseElement = document.getElementById("textResponse");

            if (fileInput.files.length === 0) {
                responseElement.textContent = "Please upload an image with text.";
                speakText("Please upload an image with text.");
                return;
            }

            const file = fileInput.files[0];
            const reader = new FileReader();

            reader.onload = async function () {
                const base64Image = reader.result.split(",")[1];

                const requestBody = {
                    requests: [
                        {
                            image: { content: base64Image },
                            features: [{ type: "TEXT_DETECTION", maxResults: 10 }]
                        }
                    ]
                };

                const apiKey = "AIzaSyA0US6rM_BBm-OypJkKfOyn7uTfatnbw"; // Your API Key here
                const apiUrl = `https://vision.googleapis.com/v1/images:annotate?key=${apiKey}`;

                responseElement.textContent = "Processing image...";
                speakText("Processing image...");

                try {
                    const response = await fetch(apiUrl, {
                        method: "POST",
                        headers: {
                            "Content-Type": "application/json"
                        },
                        body: JSON.stringify(requestBody)
                    });

                    if (!response.ok) {
                        throw new Error("Failed to fetch text detection.");
                    }

                    const data = await response.json();
                    if (data.responses[0].textAnnotations && data.responses[0].textAnnotations.length > 0) {
                        const extractedText = data.responses[0].textAnnotations[0].description;
                        responseElement.textContent = `Extracted Text: ${extractedText}`;
                        speakText(`Extracted Text: ${extractedText}`);
                    } else {
                        throw new Error("No text detected in the image.");
                    }
                } catch (error) {
                    responseElement.textContent = error.message;
                    speakText(error.message);
                }
            };

            reader.readAsDataURL(file);
        }

        let video = document.getElementById("video");
        let intervalId = null;
        let lastSentence = "";

        // Open camera
        let stream = null;
        let currentFacingMode = 'user'; // default front camera

        async function startCamera(facingMode = 'user') {
            // If stream is active and facing mode is same, don't restart
            if (stream && currentFacingMode === facingMode) return;

            // Stop existing stream if any
            if (stream) {
                stopCamera();
            }

            currentFacingMode = facingMode;

            const constraints = { video: { facingMode: { exact: facingMode } } };
            // Try exact first, then fallback
            try {
                stream = await navigator.mediaDevices.getUserMedia(constraints);
                video.srcObject = stream;
            } catch (err) {
                // fallback without exact (some browsers don't support exact)
                try {
                    stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: facingMode } });
                    video.srcObject = stream;
                } catch (err2) {
                    // final fallback: any camera
                    try {
                        stream = await navigator.mediaDevices.getUserMedia({ video: true });
                        video.srcObject = stream;
                        speakText('Unable to switch camera precisely; using default camera.');
                    } catch (err3) {
                        alert('Camera permission denied or not available.');
                    }
                }
            }
        }

        // stop camera
        function stopCamera() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                stream = null;
                video.srcObject = null;
            }
        }


        // Convert video frame to base64
        function getBase64FromVideo() {
            const canvas = document.createElement("canvas");
            canvas.width = video.videoWidth || 640;
            canvas.height = video.videoHeight || 480;
            canvas.getContext("2d").drawImage(video, 0, 0);
            return canvas.toDataURL("image/jpeg").split(',')[1];
        }

        // Text-to-Speech
        function speak(text) {
            window.speechSynthesis.cancel(); // stop ongoing speech
            const msg = new SpeechSynthesisUtterance(text);
            msg.rate = 1;
            window.speechSynthesis.speak(msg);
        }

        // Map likelihood to words
        function mapExpression(face) {
            if (face) {
                if (face.joyLikelihood === "VERY_LIKELY" || face.joyLikelihood === "LIKELY") return "smiling";
                if (face.sorrowLikelihood === "VERY_LIKELY" || face.sorrowLikelihood === "LIKELY") return "sad";
                if (face.angerLikelihood === "VERY_LIKELY" || face.angerLikelihood === "LIKELY") return "angry";
                if (face.surpriseLikelihood === "VERY_LIKELY" || face.surpriseLikelihood === "LIKELY") return "surprised";
            }
            return "neutral";
        }

        // Get dominant color name
        function getDominantColorName(rgb) {
            const [r, g, b] = rgb;
            if (r > 200 && g < 100 && b < 100) return "red";
            if (r < 100 && g > 200 && b < 100) return "green";
            if (r < 100 && g < 100 && b > 200) return "blue";
            if (r > 200 && g > 200 && b < 100) return "yellow";
            if (r > 200 && g > 200 && b > 200) return "white";
            if (r < 50 && g < 50 && b < 50) return "black";
            if (r > 150 && g > 50 && b < 50) return "orange";
            return "unknown color";
        }

        // Crop person region and estimate shirt color
        function getShirtColor(canvas, box) {
            const ctx = canvas.getContext("2d");
            const [xmin, ymin, xmax, ymax] = [box.normalizedVertices[0].x * canvas.width, box.normalizedVertices[0].y * canvas.height,
                box.normalizedVertices[2].x * canvas.width, box.normalizedVertices[2].y * canvas.height];
            const width = Math.max(1, xmax - xmin);
            const height = Math.max(1, ymax - ymin);
            const cropHeight = Math.max(1, Math.floor(height * 0.4)); // upper body
            // Clamp values to canvas
            const sx = Math.max(0, Math.floor(xmin));
            const sy = Math.max(0, Math.floor(ymin));
            const sw = Math.min(canvas.width - sx, Math.floor(width));
            const sh = Math.min(canvas.height - sy, cropHeight);
            try {
                const imageData = ctx.getImageData(sx, sy, sw, sh);
                const data = imageData.data;
                let r = 0, g = 0, b = 0, count = 0;
                for (let i = 0; i < data.length; i += 4) {
                    r += data[i]; g += data[i + 1]; b += data[i + 2]; count++;
                }
                return getDominantColorName([r / count, g / count, b / count]);
            } catch (e) {
                return 'unknown color';
            }
        }

        // Google Vision API call
        async function analyzeImageAPI(base64Image) {
            const body = {
                requests: [
                    {
                        image: { content: base64Image },
                        features: [
                            { type: "OBJECT_LOCALIZATION" },
                            { type: "FACE_DETECTION" },
                            { type: "LABEL_DETECTION", maxResults: 10 }
                        ]
                    }
                ]
            };
            const response = await fetch(`https://vision.googleapis.com/v1/images:annotate?key=AIzaSyA0US6rM_BBm-OypJkKfOyn7uTfatnbw`, {
                method: "POST",
                body: JSON.stringify(body)
            });
            const data = await response.json();
            return data;
        }

        // Generate sentence from API response
        function generateSentence(result, canvas) {
            const objects = (result.responses[0] && result.responses[0].localizedObjectAnnotations) || [];
            const faces = (result.responses[0] && result.responses[0].faceAnnotations) || [];
            const labels = (result.responses[0] && result.responses[0].labelAnnotations) || [];

            let sentence = "";
            const persons = objects.filter(o => o.name.toLowerCase() === "person");

            if (persons.length) {
                sentence += `I see ${persons.length} person${persons.length > 1 ? "s" : ""}. `;
                persons.forEach((p, i) => {
                    const faceExp = mapExpression(faces[i]);
                    const color = getShirtColor(canvas, p.boundingPoly);
                    sentence += `Person ${i + 1} is ${faceExp} wearing a ${color} shirt. `;
                });
            }

            const otherObjects = objects.filter(o => o.name.toLowerCase() !== "person");
            if (otherObjects.length) {
                sentence += "I also see " + otherObjects.map(o => o.name).join(", ") + ". ";
            }

            if (!objects.length && labels.length) {
                sentence += "I see " + labels.map(l => l.description).join(", ") + ". ";
            }

            return sentence.trim();
        }

        // Start detection
        document.getElementById("startBtn").onclick = () => {
            if (intervalId) return; // already running
            lastSentence = ""; // reset lastSentence for fresh start
            speakText("Starting live description.");
            intervalId = setInterval(async () => {
                const base64 = getBase64FromVideo();
                try {
                    const result = await analyzeImageAPI(base64);
                    const canvas = document.createElement("canvas");
                    canvas.width = video.videoWidth || 640;
                    canvas.height = video.videoHeight || 480;
                    canvas.getContext("2d").drawImage(video, 0, 0);
                    const sentence = generateSentence(result, canvas);

                    if (sentence && sentence !== lastSentence) {
                        speakText(sentence);
                        lastSentence = sentence;
                    }
                } catch (e) { console.log("Google Vision API error:", e); }
            }, 5000);
        };

        // Stop detection
        document.getElementById("stopBtn").onclick = () => {
            if (intervalId) {
                clearInterval(intervalId);
                intervalId = null;
            }

            stopCamera(); // ✅ camera off
            window.speechSynthesis.cancel();
            lastSentence = "";
            speakText("Stopped live description.");
        };

        // Back / Front camera buttons behavior
        document.getElementById('backCamBtn').addEventListener('click', async () => {
            await startCamera('environment');
            speakText('Switched to back camera');
        });

        document.getElementById('frontCamBtn').addEventListener('click', async () => {
            await startCamera('user');
            speakText('Switched to front camera');
        });

        // Stop Audio for Blind Users
        function stopAudio() {
            window.speechSynthesis.cancel();
        }

        // Function to get real-time location and speak it aloud
        function speakText(text, language = 'en-US') {
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = language;
            window.speechSynthesis.speak(utterance);
        }

        function getRealTimeLocation() {
            const responseElement = document.getElementById("locationResponse");

            // Check if Geolocation is supported
            if (!navigator.geolocation) {
                const errorMessage = "Geolocation is not supported by your browser.";
                responseElement.textContent = errorMessage;
                speakText(errorMessage);
                return;
            }

            // Start location detection
            responseElement.textContent = "Detecting location...";
            speakText("Detecting location...");

            navigator.geolocation.getCurrentPosition(
                async (position) => {
                    const latitude = position.coords.latitude;
                    const longitude = position.coords.longitude;

                    // Use reverse geocoding to get address (using a geocoding API)
                    const apiKey = "AIzaSyA0US6rM_BBm-OypJkKfOyn7uTfatnbw"; // Replace with your geocoding API key
                    const apiUrl = `https://maps.googleapis.com/maps/api/geocode/json?latlng=${latitude},${longitude}&key=${apiKey}`;

                    try {
                        const response = await fetch(apiUrl);
                        if (!response.ok) {
                            throw new Error("Failed to fetch location details.");
                        }

                        const data = await response.json();
                        if (data.status === "OK" && data.results.length > 0) {
                            const address = data.results[0].formatted_address;
                            const locationMessage = `You are currently at: ${address}`;
                            responseElement.textContent = locationMessage;
                            speakText(locationMessage);
                        } else {
                            throw new Error("Unable to determine the location address.");
                        }
                    } catch (error) {
                        responseElement.textContent = error.message;
                        speakText(error.message);
                    }
                },
                (error) => {
                    let errorMessage = "Unable to retrieve your location.";
                    if (error.code === error.PERMISSION_DENIED) {
                        errorMessage = "Location access denied. Please allow location permissions.";
                    } else if (error.code === error.POSITION_UNAVAILABLE) {
                        errorMessage = "Location information is unavailable.";
                    } else if (error.code === error.TIMEOUT) {
                        errorMessage = "The request to get your location timed out.";
                    }
                    responseElement.textContent = errorMessage;
                    speakText(errorMessage);
                }
            );
        }

        // Text-to-Speech for Mute Users
        function convertTextToSpeech() {
            const language = document.getElementById("languageSelectMute").value;
            const text = document.getElementById("textInput").value.trim();
            if (!text) {
                alert("Please enter some text.");
                return;
            }
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = language;
            window.speechSynthesis.speak(utterance);
        }

        function presetSpeak(message) {
            const speech = new SpeechSynthesisUtterance(message);
            window.speechSynthesis.speak(speech);
        }

        let recognition; // Declare recognition instance globally

        // Start Speech-to-Text for Deaf Users
        function startSpeechToText() {
            const language = document.getElementById("languageSelectDeaf").value;
            const outputElement = document.getElementById("speechOutput");

            // Initialize SpeechRecognition only if not already initialized
            if (!recognition) {
                recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
                recognition.lang = language;
                recognition.interimResults = true; // To get intermediate results
                recognition.continuous = true; // To keep it running continuously

                recognition.onresult = (event) => {
                    const transcript = Array.from(event.results)
                        .map(result => result[0].transcript)
                        .join('');
                    outputElement.textContent = transcript; // Update the output with the transcript
                };

                recognition.onerror = (event) => {
                    console.error("Speech recognition error:", event.error);
                    outputElement.textContent = `Error: ${event.error}`;
                };
            }

            recognition.start(); // Start recognition
            outputElement.textContent = "Listening for speech...";
        }

        // Stop Speech-to-Text
        function stopSpeechToText() {
            if (recognition) {
                recognition.stop(); // Stop recognition manually
                recognition = null; // Reset the recognition instance
                document.getElementById("speechOutput").textContent = "Speech recognition stopped.";
            }
        }

    </script>


    <script src="https://www.gstatic.com/dialogflow-console/fast/messenger/bootstrap.js?v=1"></script>
    <df-messenger
      chat-icon="https:&#x2F;&#x2F;encrypted-tbn0.gstatic.com&#x2F;images?q=tbn:ANd9GcT0cblfe_iPMtmlVmK2TSYhX8d_SWzjgvH9nw&amps"
      chat-title="VitaAI"
      agent-id="66defd9c-f98f-4d8d-999b-c167890813f2"
      language-code="en"
    ></df-messenger>

</body>

</html>
